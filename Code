# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.decomposition import PCA
from scipy.sparse import csr_matrix
# from gensim.models import Word2Vec
# import joblib

import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import SGD
from keras.constraints import max_norm
from keras.wrappers.scikit_learn import KerasClassifier

import tensorflow as tf
from tensorflow.keras.layers import Dropout
from tensorflow.keras import regularizers
df = pd.read_csv('mental_health.csv')
df.head()
df.shape

# Check data types
df.dtypes

# Check missing values
df.isnull().sum()

# Check the distribution of mental health status
label_0_count = df[df['label']==0].shape[0]
label_1_count = df[df['label']==1].shape[0]
print("Number of entries with label = 0: {}".format(label_0_count))
print("Number of entries with label = 1: {}".format(label_1_count))

# Vectorize "text" using CountVectorizer
# Documentation 1: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
# Documentation 2: https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/

vectorizer = CountVectorizer()
X = vectorizer.fit(df['text'])
print("Vocabulary: ", vectorizer.vocabulary_)

features = vectorizer.transform(df['text'])
features = features.toarray()

print(features.shape)

# Split data into training set and testing set
x_train, x_test, y_train, y_test = train_test_split(features, df['label'], train_size=0.8, random_state=0)
print("Number of entries in the training set: {}".format(x_train.shape[0]))
print("Number of entries in the testing set: {}".format(x_test.shape[0]))

# Get the vocabulary from the CountVectorizer object
vocabulary = vectorizer.get_feature_names_out()

# Sum the counts of each word across all documents
word_counts = np.sum(features, axis=0)

# Sort the words in descending order of frequency
sorted_idx = np.argsort(word_counts)[::-1]

# Get the top 20 words by frequency
top_words = [vocabulary[i] for i in sorted_idx[:20]]
top_counts = [word_counts[i] for i in sorted_idx[:20]]

# Plot the frequency of the top 20 words
plt.barh(top_words, top_counts)
plt.title('Frequency of Top 20 Words')
plt.xlabel('Frequency')
plt.ylabel('Word')
plt.show()

y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

# csr_matrix compress sparse matrix
x_train_csr = csr_matrix(x_train)
x_test_csr = csr_matrix(x_test)

reg = LogisticRegression(max_iter=1000)
reg.fit(x_train_csr, y_train)

# predict on test data
y_pred = reg.predict(x_test_csr)

accuracy_score(y_pred, y_test)

# Feature selection with PCA

# Define the number of components to keep
n_components = 1000

# Perform PCA feature reduction
pca = PCA(n_components=n_components)
x_train_selected = pca.fit_transform(x_train)
x_test_selected = pca.transform(x_test)

# Save the PCA model and transformed data
joblib.dump(pca, 'pca_model.pkl')
np.save('x_train_pca.npy', x_train_selected)
np.save('x_test_pca.npy', x_test_selected)

# Load the saved PCA model
pca = joblib.load('pca_model.pkl')

# Load the saved transformed data
x_train_selected = np.load('x_train_pca.npy')
x_test_selected = np.load('x_test_pca.npy')

reg = LogisticRegression(max_iter=1000)
reg.fit(x_train_selected, y_train)

# predict on test data
y_pred = reg.predict(x_test_selected)

accuracy_score(y_pred, y_test)

# Random Forest

# Define the parameter grid for the random forest classifier
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [100, 200, 300],
    'min_samples_split': [50, 100, 200, 300]
}

# Create a random forest classifier object
rfc = RandomForestClassifier()

# Create a grid search object with cross-validation
grid_search = GridSearchCV(rfc, param_grid, cv=5)

# Fit the grid search object on the training data
grid_search.fit(x_train_selected, y_train)

# Print the best hyperparameters found by the grid search
print("Best hyperparameters:", grid_search.best_params_)

# Use the best hyperparameters to train a random forest classifier on the training data
best_rfc = RandomForestClassifier(**grid_search.best_params_)
best_rfc.fit(x_train_selected, y_train)

# Use the classifier to make predictions on the testing data
predictions = best_rfc.predict(x_test_selected)

# Evaluate the best classifier on the testing data
accuracy = best_rfc.score(x_test_selected, y_test)
print("Accuracy:", accuracy)

# Print the confusion matrix
cm = confusion_matrix(y_test, predictions)
print('Confusion matrix:\n', cm)

# Nerual Network

model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(x_train_selected.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# change learning rate
model.compile(loss='binary_crossentropy',
              optimizer=SGD(learning_rate=0.001),
              metrics=['accuracy'])

# train model
model.fit(x_train_selected, y_train, epochs=50, batch_size=32,
          validation_data=(x_test_selected, y_test))

# evaluate model on test set
test_loss, test_acc = model.evaluate(x_test_selected, y_test)
print('Test accuracy:', test_acc)

# Random Froest with Word2Vec

train_data, test_data, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=0)

# Train a Word2vec model on the training data
corpus = [doc.split() for doc in train_data]
model = Word2Vec(corpus, vector_size=100, window=5, min_count=5, workers=4)

# Define a function to generate feature vectors for each comment
def get_vector(text):
    words = text.split()
    vec = np.zeros((100,))
    count = 0
    for word in words:
        if word in model.wv.key_to_index:
            vec += model.wv[word]
            count += 1
    if count > 0:
        return vec / count
    else:
        return vec
        
  # Generate feature vectors for the training and testing data
train_features = np.vstack([get_vector(text) for text in train_data])
test_features = np.vstack([get_vector(text) for text in test_data])

# Train a random forest classifier on the training data
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(train_features, train_labels)

# Use the classifier to make predictions on the testing data
predictions = rfc.predict(test_features)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(test_labels, predictions)
print('Accuracy:', accuracy)

# Print the confusion matrix
cm = confusion_matrix(test_labels, predictions)
print('Confusion matrix:\n', cm)

model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(train_features.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

# change learning rate
model.compile(loss='binary_crossentropy',
              optimizer=SGD(learning_rate=0.001),
              metrics=['accuracy'])

# train model
model.fit(train_features, train_labels, epochs=50, batch_size=32,
          validation_data=(test_features, y_test))

# evaluate model on test set
test_loss, test_acc = model.evaluate(test_features, y_test)
print('Test accuracy:', test_acc)

# BERT with NN

# Load training and testing sets
train_df = pd.read_csv('train_df_distilbert.csv')
test_df = pd.read_csv('test_df_distilbert.csv')
# Load the saved embeddings from files
train_embeddings_distil = np.load('train_embeddings_distilbert_1500rows.npy')
test_embeddings_distil = np.load('test_embeddings_distilbert_1500rows.npy')

# Define the neural network model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(768,), 
                       kernel_regularizer=regularizers.l2(0.001)),
    Dropout(0.1),
    keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    Dropout(0.1),
    keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.1),
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile the neural network model
model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),
              metrics=['accuracy'])

# Train the neural network model
model.fit(train_embeddings_distil, train_df['label'], epochs=50, batch_size=32,
          validation_data=(test_embeddings_distil, test_df['label']))

# Evaluate the neural network model on the test data
test_loss, test_acc = model.evaluate(test_embeddings_distil, test_df['label'])
print('Test accuracy:', test_acc)

# Load training and testing sets
train_df = pd.read_csv('train_df_bert.csv')
test_df = pd.read_csv('test_df_bert.csv')

# Load the saved embeddings from files
train_embeddings_non_distil = np.load('train_embeddings_bert_1500rows.npy')
test_embeddings_non_distil = np.load('test_embeddings_bert_1500rows.npy')


# Define the neural network model with L2 regularization and dropout
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(768,), 
                       kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.2),
    keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.1)),
    Dropout(0.2),
    keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.1)),
    Dropout(0.2),
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile the neural network model
model.compile(loss='binary_crossentropy',
              optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),
              metrics=['accuracy'])

# Train the neural network model
model.fit(train_embeddings_non_distil, train_df['label'], epochs=100, batch_size=32,
         validation_data=(test_embeddings_non_distil, test_df['label']))

# Evaluate the neural network model on the test data
test_loss, test_acc = model.evaluate(test_embeddings_non_distil, test_df['label'])
print('Test accuracy:', test_acc)
